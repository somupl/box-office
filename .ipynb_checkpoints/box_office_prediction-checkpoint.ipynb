{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_squared_log_error\n",
    "import json\n",
    "import ast\n",
    "import eli5\n",
    "from catboost import CatBoostRegressor\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#import shap\n",
    "#stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_date(x):\n",
    "    \"\"\"\n",
    "    Fixes dates which are in 20xx\n",
    "    \"\"\"\n",
    "    year = x.split('/')[2]\n",
    "    if int(year) <= 19:\n",
    "        return x[:-2] + '20' + year\n",
    "    else:\n",
    "        return x[:-2] + '19' + year\n",
    "    \n",
    "def text_to_dict(df):\n",
    "    dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
    "            'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
    "    for column in dict_columns:\n",
    "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engg function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fea_engg(df, top_genres, top_companies, top_countries, top_languages, top_keywords, top_cast_names, top_cast_characters,\n",
    "            top_crew_names,top_crew_jobs,top_crew_departments):\n",
    "\n",
    "    \n",
    "    # -------------------- step 2 - collection ------------------------------ #\n",
    "    # if it has collection name, get the collection name else add 0\n",
    "    df['collection_name'] = df['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\n",
    "    # if it has collection, give length else return 0\n",
    "    df['has_collection'] = df['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n",
    "    # drop the column we used\n",
    "    df = df.drop(['belongs_to_collection'], axis=1)\n",
    "    \n",
    "    # -------------------- step 3 - genres ------------------------------ #\n",
    "    # get count of all genres\n",
    "    df['num_genres'] = df['genres'].apply(lambda x: len(x) if x != {} else 0)\n",
    "    # join all the genres together to form a word. \n",
    "    df['all_genres'] = df['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "    # form seperate column for each genres and set one if the genere is present\n",
    "    for g in top_genres:\n",
    "        df['genre_' + g] = df['all_genres'].apply(lambda x: 1 if g in x else 0)\n",
    "    # drop the column we used\n",
    "    df = df.drop(['genres'], axis=1)\n",
    "    \n",
    "    # -------------------- step 4 - companies ------------------------------ #\n",
    "    # get count of all companies\n",
    "    df['num_companies'] = df['production_companies'].apply(lambda x: len(x) if x != {} else 0)\n",
    "    # join all companies\n",
    "    df['all_production_companies'] = df['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "    # form seperate column for each top companies and set one if that company is present\n",
    "    for g in top_companies:\n",
    "        df['production_company_' + g] = df['all_production_companies'].apply(lambda x: 1 if g in x else 0)\n",
    "    # drop the column we used\n",
    "    df = df.drop(['production_companies', 'all_production_companies'], axis=1)\n",
    "\n",
    "    # -------------------- step 5 - coutries ------------------------------ #\n",
    "    # get count of all countries\n",
    "    df['num_countries'] = df['production_countries'].apply(lambda x: len(x) if x != {} else 0)\n",
    "    # join all countries. \n",
    "    df['all_countries'] = df['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "    # form seperate column for each top countries and set one if that country is present\n",
    "    for g in top_countries:\n",
    "        df['production_country_' + g] = df['all_countries'].apply(lambda x: 1 if g in x else 0)\n",
    "    # drop the column we used\n",
    "    df = df.drop(['production_countries', 'all_countries'], axis=1)\n",
    "    \n",
    "    # -------------------- step 6 - lang ------------------------------ #\n",
    "    # get count of all lang\n",
    "    df['num_languages'] = df['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\n",
    "    # join all lang.\n",
    "    df['all_languages'] = df['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "    # form seperate column for each top lang and set one if that lang is present\n",
    "    for g in top_languages:\n",
    "        df['language_' + g] = df['all_languages'].apply(lambda x: 1 if g in x else 0)\n",
    "    # drop the column we used\n",
    "    df = df.drop(['spoken_languages', 'all_languages'], axis=1)\n",
    "\n",
    "    # -------------------- step 7 - keyword ------------------------------ #\n",
    "    # follow same procedure for keyword. you got the idea.\n",
    "    df['num_Keywords'] = df['Keywords'].apply(lambda x: len(x) if x != {} else 0)\n",
    "    df['all_Keywords'] = df['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "    for g in top_keywords:\n",
    "        df['keyword_' + g] = df['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n",
    "    df = df.drop(['Keywords', 'all_Keywords'], axis=1)\n",
    "\n",
    "    # -------------------- step 8 - cast ------------------------------ #\n",
    "    df['num_cast'] = df['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
    "    for g in top_cast_names:\n",
    "        df['cast_name_' + g] = df['cast'].apply(lambda x: 1 if g in str(x) else 0)\n",
    "    df['genders_0_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n",
    "    df['genders_1_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n",
    "    df['genders_2_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n",
    "    for g in top_cast_characters:\n",
    "        df['cast_character_' + g] = df['cast'].apply(lambda x: 1 if g in str(x) else 0)\n",
    "    df = df.drop(['cast'], axis=1)\n",
    "\n",
    "    \n",
    "    # -------------------- step 9 - crew gender, job, dept, name ------------------------------ #\n",
    "    df['num_crew'] = df['crew'].apply(lambda x: len(x) if x != {} else 0)\n",
    "    for g in top_crew_names:\n",
    "        df['crew_name_' + g] = df['crew'].apply(lambda x: 1 if g in str(x) else 0)\n",
    "    df['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n",
    "    df['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n",
    "    df['genders_2_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n",
    "    for j in top_crew_jobs:\n",
    "        df['jobs_' + j] = df['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\n",
    "    for j in top_crew_departments:\n",
    "        df['departments_' + j] = df['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n",
    "    df = df.drop(['crew'], axis=1)\n",
    "\n",
    "    # -------------------- step 10 - convert budget to log ------------------------------ #\n",
    "    df['log_budget'] = np.log1p(df['budget'])\n",
    "\n",
    "    # -------------------- step 11 - create has homepage varaible ------------------------------ #\n",
    "    df['has_homepage'] = 0\n",
    "    df.loc[df['homepage'].isnull() == False, 'has_homepage'] = 1\n",
    "    \n",
    "    # -------------------- step 12 - get release date and convert it to year with 4 character and date time------------------------------ #\n",
    "    df['release_date'] = df['release_date'].apply(lambda x: '2/20/2015' if pd.isna(x) else fix_date(x))\n",
    "    df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "    \n",
    "    # -------------------- step 13 - get release date split it into several parts ------------------------------ #\n",
    "    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n",
    "    for part in date_parts:\n",
    "        part_col = 'release_date' + \"_\" + part\n",
    "        df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n",
    "        \n",
    "    # -------------------- step 14 - drop columns that are not used ------------------------------ #\n",
    "    df = df.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status'], axis=1)\n",
    "    \n",
    "    # -------------------- step 15 - get length and number of words in title, tagline, overview, original_title for both test and train------------------------------ #\n",
    "    for col in ['title', 'tagline', 'overview', 'original_title']:\n",
    "        df['len_' + col] = df[col].fillna('').apply(lambda x: len(str(x)))\n",
    "        df['words_' + col] = df[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n",
    "        df = df.drop(col, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data & get top informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# -------------------- step 1 ------------------------------ #\n",
    "# get dict columns which are like strings and convert to dict columns.\n",
    "\n",
    "train = text_to_dict(train)\n",
    "test = text_to_dict(test)\n",
    "\n",
    "# ---------------- get list of generes and top geners -------------------#\n",
    "list_of_genres = list(train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n",
    "top_genres = [m[0] for m in Counter([i for j in list_of_genres for i in j]).most_common(15)]\n",
    "\n",
    "# ---------------- get list of companies and top companies -------------------#\n",
    "list_of_companies = list(train['production_companies'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n",
    "top_companies = [m[0] for m in Counter([i for j in list_of_companies for i in j]).most_common(30)]\n",
    "\n",
    "# ---------------- get list of countries and top coutries -------------------#\n",
    "list_of_countries = list(train['production_countries'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n",
    "top_countries = [m[0] for m in Counter([i for j in list_of_countries for i in j]).most_common(25)]\n",
    "\n",
    "# ---------------- get list of lang and top lang -------------------#\n",
    "list_of_languages = list(train['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n",
    "top_languages = [m[0] for m in Counter([i for j in list_of_languages for i in j]).most_common(30)]\n",
    "\n",
    "# ---------------- get list of keywords and top keywords -------------------#\n",
    "list_of_keywords = list(train['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n",
    "top_keywords = [m[0] for m in Counter([i for j in list_of_keywords for i in j]).most_common(30)]\n",
    "\n",
    "# ---------------- get list of cast name and top cast name -------------------#\n",
    "list_of_cast_names = list(train['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n",
    "top_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(15)]\n",
    "\n",
    "# ---------------- get list of cast character and top cast character -------------------#\n",
    "list_of_cast_characters = list(train['cast'].apply(lambda x: [i['character'] for i in x] if x != {} else []).values)\n",
    "top_cast_characters = [m[0] for m in Counter([i for j in list_of_cast_characters for i in j]).most_common(15)]\n",
    "\n",
    "# ---------------- get list of crew names and top crew names -------------------#\n",
    "list_of_crew_names = list(train['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n",
    "top_crew_names = [m[0] for m in Counter([i for j in list_of_crew_names for i in j]).most_common(15)]\n",
    "\n",
    "# ---------------- get list of crew jobs and top crew jobs -------------------#\n",
    "list_of_crew_jobs = list(train['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)\n",
    "top_crew_jobs = [m[0] for m in Counter([i for j in list_of_crew_jobs for i in j]).most_common(15)]\n",
    "\n",
    "# ---------------- get list of crew dept and top crew dept -------------------#\n",
    "list_of_crew_departments = list(train['crew'].apply(lambda x: [i['department'] for i in x] if x != {} else []).values)\n",
    "top_crew_departments = [m[0] for m in Counter([i for j in list_of_crew_departments for i in j]).most_common(15)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fea_engg(train, top_genres, top_companies, top_countries, top_languages, top_keywords, top_cast_names, top_cast_characters,\n",
    "                top_crew_names,top_crew_jobs,top_crew_departments)\n",
    "\n",
    "test = fea_engg(test, top_genres, top_companies, top_countries, top_languages, top_keywords, top_cast_names, top_cast_characters,\n",
    "               top_crew_names,top_crew_jobs,top_crew_departments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        90.0\n",
       "1        65.0\n",
       "2       100.0\n",
       "3       130.0\n",
       "4        92.0\n",
       "5       121.0\n",
       "6       119.0\n",
       "7        77.0\n",
       "8       120.0\n",
       "9        92.0\n",
       "10       88.0\n",
       "11      112.0\n",
       "12      109.0\n",
       "13       88.0\n",
       "14      114.0\n",
       "15      100.0\n",
       "16      101.0\n",
       "17      119.0\n",
       "18      123.0\n",
       "19      194.0\n",
       "20      100.0\n",
       "21      101.0\n",
       "22      105.0\n",
       "23       99.0\n",
       "24      105.0\n",
       "25      128.0\n",
       "26      100.0\n",
       "27      128.0\n",
       "28       84.0\n",
       "29       94.0\n",
       "        ...  \n",
       "4368     98.0\n",
       "4369    108.0\n",
       "4370    100.0\n",
       "4371    119.0\n",
       "4372    126.0\n",
       "4373     98.0\n",
       "4374    130.0\n",
       "4375    101.0\n",
       "4376    144.0\n",
       "4377     83.0\n",
       "4378    122.0\n",
       "4379     96.0\n",
       "4380     86.0\n",
       "4381    225.0\n",
       "4382    100.0\n",
       "4383    115.0\n",
       "4384    114.0\n",
       "4385     93.0\n",
       "4386    104.0\n",
       "4387     90.0\n",
       "4388     81.0\n",
       "4389     84.0\n",
       "4390     85.0\n",
       "4391    110.0\n",
       "4392    126.0\n",
       "4393    118.0\n",
       "4394     95.0\n",
       "4395    129.0\n",
       "4396    100.0\n",
       "4397     85.0\n",
       "Name: runtime, Length: 4398, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------- step 16 - number of unique value in that column is just 1, then drop that column in both train and test ------------------------------ #\n",
    "for col in train.columns:\n",
    "    if train[col].nunique() == 1:\n",
    "        #print(col)\n",
    "        train = train.drop([col], axis=1)\n",
    "        test = test.drop([col], axis=1)\n",
    "        \n",
    "# -------------------- step 17 - label encode test and train for below 3 columns ------------------------------ #\n",
    "for col in ['original_language', 'collection_name', 'all_genres']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train[col].fillna('')) + list(test[col].fillna('')))\n",
    "    train[col] = le.transform(train[col].fillna('').astype(str))\n",
    "    test[col] = le.transform(test[col].fillna('').astype(str))\n",
    "    \n",
    "# ---------------- step 18 - fix power 6 error in train data --------------------\n",
    "power_six = train.id[train.budget > 1000][train.revenue < 100]\n",
    "\n",
    "for k in power_six :\n",
    "    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000\n",
    "    \n",
    "# ---------------- step 19 - fix run time nan --------------------\n",
    "train['runtime'] = train['runtime'].fillna(train['runtime'].mean())\n",
    "test['runtime'] = test['runtime'].fillna(train['runtime'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form X and y, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['id', 'revenue'], axis=1)\n",
    "y = np.log1p(train['revenue'])\n",
    "X_test = test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[358]\ttraining's rmse: 1.63645\tvalid_1's rmse: 1.82652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.9, bagging_freq=1, bagging_seed=11,\n",
       "       boosting='gbdt', boosting_type='gbdt', class_weight=None,\n",
       "       colsample_bytree=1.0, feature_fraction=0.9, importance_type='split',\n",
       "       lambda_l1=0.2, learning_rate=0.01, max_depth=5, metric='rmse',\n",
       "       min_child_samples=20, min_child_weight=0.001, min_data_in_leaf=20,\n",
       "       min_split_gain=0.0, n_estimators=20000, n_jobs=-1, nthread=4,\n",
       "       num_leaves=30, objective='regression', random_state=None,\n",
       "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=0, verbosity=-1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'num_leaves': 30,\n",
    "         'min_data_in_leaf': 20,\n",
    "         'objective': 'regression',\n",
    "         'max_depth': 5,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.2,\n",
    "         \"verbosity\": -1}\n",
    "model1 = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n",
    "model1.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n",
    "        verbose=1000, early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0443409926791474"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LinearRegression().fit(X_train, y_train)\n",
    "np.sqrt(mean_squared_error(y_valid, model2.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Ridge with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ej7hz6f\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:125: LinAlgWarning:\n",
      "\n",
      "Ill-conditioned matrix (rcond=7.46425e-18): result may not be accurate.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9976966389581265"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'alpha': np.logspace(-4, 5)}\n",
    "gs = GridSearchCV(Ridge(), param_grid=params, cv=5, n_jobs=5).fit(X_train, y_train)\n",
    "np.sqrt(mean_squared_error(y_valid, gs.predict(X_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create answer file after training with entire train data using the best selected model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n",
    "#model = LinearRegression()\n",
    "#model = Ridge(**gs.best_params_)\n",
    "model.fit(X, y)\n",
    "\n",
    "test_id = test.id\n",
    "ans = pd.Series(np.exp(model.predict(X_test)),name = 'revenue')\n",
    "pd.concat([test_id, ans], axis=1).to_csv(index = False, path_or_buf  = 'data/result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
